{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87a844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70aaae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_next_page(driver):\n",
    "    \"\"\"\n",
    "    This function is responsible for clicking the 'next' button to navigate to the next page.\n",
    "    We are using xpath here since static id is missing.\n",
    "    For locating the exact element, below logic is used:\n",
    "\n",
    "    1. extract all buttons in the footer via xpath.\n",
    "    2. extract the last element from the list of buttons.\n",
    "    3. this is done since the array length changes when we navigate to page 4 onwards.\n",
    "    4. 'next' button would always be the last in the list so we fetch it via -1 index.\n",
    "    \"\"\"\n",
    "\n",
    "    buttons = driver.find_elements(By.XPATH,\n",
    "                                   '/html/body/div/div[2]/div[2]/div/div/div[1]/div[2]/div/div/div[2]/div[1]/div[2]/div/button')\n",
    "    next_button = buttons[-1]\n",
    "    next_disabled = next_button.get_property(\"disabled\")\n",
    "\n",
    "    print('Next page button disabled? {disabled}'.format(disabled=next_disabled))\n",
    "    if next_button.is_enabled() or not next_disabled:\n",
    "        next_button.click()\n",
    "        time.sleep(2)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46d1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_page_size(driver, count):\n",
    "    \"\"\"\n",
    "    This function is responsible for changing the default number of entries on the page.\n",
    "    By default we have 10 entries, that could lead to alot of pages to be scraped.\n",
    "    We change this to 50 (by clicking on it twice).\n",
    "    This reduces the number of pages to be extracted.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(count):\n",
    "        driver.find_element(By.XPATH, '/html/body/div/div[2]/div[2]/div/div/div[1]/div[1]/button[3]').click()\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552bd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_for_job(job):\n",
    "    \"\"\"\n",
    "    This function is responsible for extracting the metadata for each job entry.\n",
    "    metadata consists of title, description and job URL.\n",
    "\n",
    "    :param job: DOM element for the job entry.\n",
    "    :return: dict containing metadata info.\n",
    "    \"\"\"\n",
    "    metadata = None\n",
    "    job = job.find(\"div\", class_=\"job-text-div\")\n",
    "    try:\n",
    "        title = job.find(\"a\", class_=\"resultUrl hyphenate\").text\n",
    "        desc = job.find(\"div\", class_=\"snippet hyphenate\").text\n",
    "        job_url = base_url + job.find(\"a\", class_=\"resultUrl hyphenate\").get(\"href\")\n",
    "        metadata = {\"title\": title, \"description\": desc, \"job_url\": job_url}\n",
    "    except:\n",
    "        pass  # silently ignore any parsing failure\n",
    "    finally:\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8818ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction for page 1\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 2\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 3\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 4\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 5\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 6\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 7\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 8\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 9\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 10\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 11\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 12\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 13\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 14\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 15\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 16\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 17\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 18\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 19\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 20\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 21\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 22\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 23\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 24\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 25\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 26\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 27\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 28\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 29\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 30\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 31\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 32\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 33\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 34\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 35\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 36\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 37\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 38\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 39\n",
      "Next page button disabled? False\n",
      "Starting extraction for page 40\n",
      "Next page button disabled? True\n",
      "Total entries extracted: 1991\n"
     ]
    }
   ],
   "source": [
    "def extract_metadata_for_page(driver):\n",
    "    \"\"\"\n",
    "    This function is responsible for extracting the metadata for all the jobs listed on this page.\n",
    "    :param driver:\n",
    "    :return: dataframe with metadata for all job entries found on the page.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=['title', 'description', 'job_url'])\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find(\"div\", class_=\"joblist\").findChildren()\n",
    "\n",
    "    for job in jobs:\n",
    "        metadata = extract_metadata_for_job(job)\n",
    "        if metadata:\n",
    "            df = df.append(metadata, ignore_index=True)\n",
    "    return df\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    base_url = 'https://www.metajob.at' # This is needed to concat with job URL\n",
    "    it_url = 'https://www.metajob.at/banking'\n",
    "\n",
    "    try:\n",
    "        next_page_available = True\n",
    "        counter = 1\n",
    "        df = pd.DataFrame(columns=['title', 'description', 'job_url'])\n",
    "\n",
    "        driver = driver = webdriver.Chrome(executable_path='https://www.metajob.at')\n",
    "        driver.maximize_window()\n",
    "        driver.get(it_url)\n",
    "        time.sleep(3)  # TO-DO remove all hard-coded sleep with selenium way of waiting.\n",
    "        click_page_size(driver, 2)\n",
    "\n",
    "        while next_page_available:\n",
    "            print('Starting extraction for page {page}'.format(page=counter))\n",
    "            df = df.append(extract_metadata_for_page(driver))\n",
    "            next_page_available = click_next_page(driver)\n",
    "            counter += 1\n",
    "\n",
    "        print('Total entries extracted: {entries}'.format(entries=df.shape[0]))\n",
    "        df.to_csv('banking.csv', index=False)\n",
    "\n",
    "    except:\n",
    "        print('Issue in opening web page')\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0c2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
